{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape The Website : https://scrapsfromtheloft.com/stand-up-comedy-scripts/ -> Preprocess and Do Some Cleaning -> Save To csv for further analysis ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import re\n",
    "import string\n",
    "from langdetect import detect\n",
    "\n",
    "import imdb\n",
    "imdb = imdb.IMDb()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(x):\n",
    "    html_data= requests.get(x).text\n",
    "    link_data = BeautifulSoup(html_data,\"lxml\")\n",
    "    result = [x.get('href') for x in link_data.find(class_=\"elementor-section elementor-top-section elementor-element elementor-element-b70b8d7 elementor-section-boxed elementor-section-height-default elementor-section-height-default\").find_all(\"a\")] \n",
    "    return result\n",
    "\n",
    "def scrape_tags(x):\n",
    "    html_data= requests.get(x).text\n",
    "    link_data = BeautifulSoup(html_data,\"lxml\")\n",
    "    result = [x.text for x in link_data.find(class_=\"elementor-section elementor-top-section elementor-element elementor-element-b70b8d7 elementor-section-boxed elementor-section-height-default elementor-section-height-default\").find_all(\"h3\")] \n",
    "    return result\n",
    "\n",
    "def scrape_transcript(x):\n",
    "    html_data= requests.get(x).text\n",
    "    link_data = BeautifulSoup(html_data,\"lxml\")\n",
    "    result = [x.text for x in link_data.find(class_=\"elementor-element elementor-element-74af9a5b elementor-widget elementor-widget-theme-post-content\").find_all(\"p\")] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = scrape_links(\"https://scrapsfromtheloft.com/stand-up-comedy-scripts/\")\n",
    "tags = scrape_tags(\"https://scrapsfromtheloft.com/stand-up-comedy-scripts/\")\n",
    "transcript = [scrape_transcript(x) for x in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_tag = pd.DataFrame(tags, columns=[\"Tag\"])\n",
    "frame_link = pd.DataFrame(links, columns=[\"URL\"])\n",
    "data = {\n",
    "    \"Tag\": tags,\n",
    "    \"URL\": links,\n",
    "    \"Raw Transcript\": transcript\n",
    "}\n",
    "frame = pd.DataFrame(data)\n",
    "frame.insert(loc=0, column='S No.', value=np.arange(len(frame)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unwanted characters from tags and extract name , title and year\n",
    "frame['Tag'] = frame['Tag'].map(lambda x: x.lstrip('\\n\\n\\t\\t\\t\\t').rstrip('-\\n\\n\\t\\t\\t\\t'))\n",
    "frame['Names'] = frame['Tag'].str.extract(r'([\\w\\s.]+)')\n",
    "frame['Title'] = frame['Tag'].str.extract(r'([\\w\\s\\d.:,’*?!-%]+)')\n",
    "frame['Year'] = frame['Tag'].str.extract(r'(\\d{4})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"transcripts\"):\n",
    "    os.mkdir(\"transcripts\")\n",
    "\n",
    "# Dumping individual transcripts into text files\n",
    "Sr = frame['S No.'].tolist()\n",
    "for i, c in enumerate(Sr):\n",
    "    with open(\"transcripts/\" + str(c) + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(frame['Raw Transcript'][i], file)\n",
    "\n",
    "# Load pickled transcript files\n",
    "data = {}\n",
    "for i, c in enumerate(Sr):\n",
    "    with open(\"transcripts/\" + str(c) + \".txt\", \"rb\") as file:\n",
    "        data[c] = pickle.load(file)\n",
    "\n",
    "# Function to combine text from a list of text\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "# Combine the text for each transcript into one string\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}\n",
    "\n",
    "# Create a DataFrame for the combined transcripts\n",
    "frame_trans = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "frame_trans.columns = ['Transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clean function to preprocess text\n",
    "def clean(text):\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)            # Remove text in square brackets\n",
    "    text = text.lower()                           # Convert text to lowercase\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)    # Remove punctuation\n",
    "    text = re.sub('\\n', '', text)                  # Remove newlines\n",
    "    text = re.sub('[‘’“”…]', '', text)             # Remove specific special characters\n",
    "    text = re.sub('[♪)(“”…]', '', text)            # Remove additional special characters\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)            # Remove words containing numbers\n",
    "    return text\n",
    "\n",
    "# Apply the clean function to the \"Transcript\" column and create a new DataFrame for preprocessed transcripts\n",
    "frame_trans = pd.DataFrame(frame_trans.Transcript.apply(clean))\n",
    "\n",
    "# Concatenate the preprocessed transcripts with the original DataFrame 'frame' along the columns\n",
    "frame = pd.concat([frame, frame_trans], axis=1)\n",
    "\n",
    "# Drop any rows with missing values (NaN) from the DataFrame\n",
    "frame = frame.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get runtime and rating info with an IMDb api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total not found: 32\n"
     ]
    }
   ],
   "source": [
    "def get_imdb_info(titles):\n",
    "    runtime = []\n",
    "    rating = []\n",
    "    count = -1\n",
    "    errors = 0\n",
    "    \n",
    "    for i in titles:\n",
    "        count += 1\n",
    "        # Search the first 30 characters on IMDb\n",
    "        result = imdb.search_movie(i[:30])\n",
    "        try:\n",
    "            mov = imdb.get_movie(result[0].movieID, info=['main'])\n",
    "            runtime.append(int(mov.get('runtimes')[0]))\n",
    "            rating.append(mov.get('rating'))\n",
    "        except:\n",
    "            runtime.append('')\n",
    "            rating.append('')\n",
    "            errors += 1\n",
    "            # print(f'Error on index {count}, title: {i}')\n",
    "    print(f'Total not found: {errors}')\n",
    "    return pd.Series(runtime), pd.Series(rating)\n",
    "    \n",
    "runtime, rating = get_imdb_info(frame.Title)\n",
    "\n",
    "frame['runtime'] = runtime\n",
    "frame['rating'] = rating\n",
    "\n",
    "# Replace empty values with NaN\n",
    "frame = frame.replace(r'^\\s*$', np.NaN, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "en    398\n",
      "it      6\n",
      "es      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dectect language based on the first 500 characters and create a language column in the df\n",
    "frame['language'] = frame.Transcript.apply(lambda x: detect(x[:500]))\n",
    "print(frame.language.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for which transcripts are empty\n",
    "drop_indices = frame[frame.Transcript == ''].index\n",
    "frame.drop(drop_indices , inplace=True)\n",
    "frame = frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the csv file\n",
    "frame.to_csv(\"frame.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "def create_joblib(folder_path):\n",
    "    transcripts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                transcript = file.read()\n",
    "                transcripts.append(transcript)\n",
    "\n",
    "    joblib.dump(transcripts, 'transcripts_joblib')\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'D:\\\\NLP\\\\Transcript Analysis\\\\Transcript Analysis\\\\transcripts'\n",
    "create_joblib(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
