# ğŸ¤ TransNLP: Stand-Up Comedy Transcript Analysis & IMDb Rating Predictor

This project offers a comprehensive platform for analyzing stand-up comedy transcripts, leveraging Natural Language Processing (NLP) to extract valuable insights, predict content success, and visualize cultural trends. Transcripts are sourced from [**Scraps From The Loft**](https://scrapsfromtheloft.com/).

## âœ¨ Features

* **ğŸŒ Web Scraping**: Automated collection of stand-up comedy transcripts, comedian names, show titles, and associated metadata (Year, IMDb Rating, Runtime) directly from the web.

* **ğŸ§¹ Data Preprocessing**: Robust cleaning pipeline including text normalization, stopword removal, lemmatization (using spaCy), profanity detection, and feature engineering (word count, diversity).

* **ğŸ§  Topic Modeling (LDA)**: Utilizes Latent Dirichlet Allocation to uncover prevalent themes within comedy content (e.g., Politics, Relationships, Culture).

* **ğŸ“Š Content Clustering**: Groups comedy specials into thematic clusters based on their textual content, providing macro-level insights.

* **ğŸ¯ IMDb Rating Prediction**: Predicts the potential IMDb rating category (Above Average / Below Average) for new content using an ensemble classification model, powered by extracted NLP features.

* **ğŸ“ˆ Trend Analysis**: Visualizes shifts in topic popularity and audience reception over time.

* **ğŸ‘¥ Audience Insights**: Helps understand audience preferences by mapping content themes to historical performance.

* **ğŸš€ Interactive Streamlit Dashboard**: A user-friendly web application for real-time content analysis, prediction, and interactive data exploration.

## ğŸ“‚ Project Structure

transnlp/
â”œâ”€â”€ .streamlit/             # Streamlit specific configuration (e.g., config.toml)
â”‚   â””â”€â”€ config.toml
â”œâ”€â”€ data/                   # Centralized data storage
â”‚   â”œâ”€â”€ raw/                # Raw scraped data and temporary transcript files
â”‚   â”‚   â”œâ”€â”€ scraped_and_cleaned_content_data.csv
â”‚   â”‚   â””â”€â”€ transcripts_raw/ # Individual raw transcript files (pickle dumps)
â”‚   â”œâ”€â”€ processed/          # Cleaned, preprocessed, and feature-engineered data
â”‚   â”‚   â””â”€â”€ processed_content_data.csv
â”‚   â””â”€â”€ models/             # Trained machine learning and NLP models
â”‚       â”œâ”€â”€ lda_model.pkl
â”‚       â”œâ”€â”€ lda_model_dict.pkl
â”‚       â”œâ”€â”€ tfidf_vectorizer.pkl
â”‚       â”œâ”€â”€ kmeans_lda.pkl
â”‚       â”œâ”€â”€ kmeans_tfidf.pkl
â”‚       â””â”€â”€ ensemble_classifier.pkl
â”œâ”€â”€ pages/                  # Streamlit multi-page application components
â”‚   â”œâ”€â”€ 01_Content_Success_Predictor.py
â”‚   â”œâ”€â”€ 02_Audience_Insights_&_Personalization.py
â”‚   â””â”€â”€ 03_Cultural_Trend_Dashboard.py
â”œâ”€â”€ scripts/                # Standalone scripts for data acquisition and initial processing
â”‚   â”œâ”€â”€ scrape_data.py      # Script to scrape and perform initial cleaning
â”‚   â””â”€â”€ preprocess_data.py  # Script for further preprocessing and feature engineering
â”œâ”€â”€ src/                    # Core application source code
â”‚   â”œâ”€â”€ init.py
â”‚   â”œâ”€â”€ utils/              # General utility functions
â”‚   â”‚   â”œâ”€â”€ init.py
â”‚   â”‚   â”œâ”€â”€ logger.py       # Application-wide logging configuration
â”‚   â”‚   â””â”€â”€ helpers.py      # Common helper functions (text cleaning, model/data loading)
â”‚   â”œâ”€â”€ models/             # Machine learning and NLP model definitions and training logic
â”‚   â”‚   â”œâ”€â”€ init.py
â”‚   â”‚   â”œâ”€â”€ nlp_pipeline.py     # Functions for applying trained NLP pipeline on new text input
â”‚   â”‚   â”œâ”€â”€ nlp_topic_modeling.py # Script for training LDA topic model
â”‚   â”‚   â”œâ”€â”€ clustering.py       # Script for training TF-IDF & KMeans clustering models
â”‚   â”‚   â””â”€â”€ prediction_model.py   # Script for training the ensemble classification model
â”‚   â””â”€â”€ config.py           # Centralized application configurations and paths
â”œâ”€â”€ app.py                  # Main Streamlit application entry point
â”œâ”€â”€ requirements.txt        # Python package dependencies
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ .gitignore              # Specifies intentionally untracked files to ignore
â””â”€â”€ .env                    # Environment variables (e.g., API keys - not committed
## ğŸš€ Getting Started

### ğŸ”§ Installation

1.  **Clone the repository:**

    ```bash
    git clone [https://github.com/your-username/transnlp.git](https://github.com/your-username/transnlp.git)
    cd transnlp


    ```

2.  **Create and activate a virtual environment (recommended):**

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: `venv\Scripts\activate`


    ```

3.  **Install Python dependencies:**

    ```bash
    pip install -r requirements.txt


    ```

4.  **Download NLTK and spaCy data:**
    The `src/utils/helpers.py` script will attempt to download necessary NLTK data automatically on its first run if missing.
    For spaCy, download the English model:

    ```bash
    python -m spacy download en_core_web_sm


    ```

### ğŸ“ˆ Initial Setup (Data & Models)

For the application to function correctly, you need to first scrape data and train the NLP and prediction models. This process involves executing specific scripts:

1.  **Run Data Scraping & Preprocessing:**

    ```bash
    python scripts/scrape_data.py
    python scripts/preprocess_data.py


    ```

    *These scripts will populate the `data/raw/` and `data/processed/` directories with the necessary CSV files.*

2.  **Train NLP Models:**

    ```bash
    python src/models/nlp_topic_modeling.py
    python src/models/clustering.py


    ```

    *This will train and save the LDA model and its dictionary into `data/models/`.*

3.  **Train Prediction Model:**

    ```bash
    python src/models/prediction_model.py


    ```

    *This will train and save the `ensemble_classifier.pkl` model into `data/models/`.*

### â–¶ï¸ Run the Streamlit App

```bash
streamlit run app.py

```